---
title: 'Mathematics and Application of Bayesian Statistics'
author: "Mikhail Popov"
date: "January 9, 2016"
output:
  ioslides_presentation:
    keep_md: yes
    logo: ../../../../shared/wmf_logo.png
    smaller: yes
    css: style.css
    runtime: shiny
runtime: shiny
---

```{r setup, include = FALSE}
library(shiny)
```

## Notation

A *random variable* can take on different values, each with some probability.

Let $X$ and $Y$ be random variables.

- The random variable $Y|X$ is $Y$ conditional on $X$ ("Y given X")
- $p(y) = P(Y = y)$ is the probability of the random variable $Y$ being $y$
- $p(y|x) = P(Y = y~|~X = x)$ is the probability of $Y$ being $y$ given that $X = x$
- **(Optional, for future ref.)** $\mathbb{E}(X)$ is the expected value of $X$ ("the mean")
- $\mathbb{E}(X) = \sum x~p(x)$ is the mean of a discrete r.v. $X$
- $\mathbb{E}(X) = \int x~p(x)~dx$ is the mean of a continuous r.v. $X$

## Bayes' Theorem

For events $A$ and $B$, Bayes' rule states that the conditional probability of $A$ given $B$ can be expressed as:

$$P(A|B) = \frac{P(A)~P(B|A)}{P(B)}$$

## Bayesian Inference

Let $\theta$ be the unobserved quantity we're interested in (the parameter) and $y$ be the observed quantity (the data). Then $$p(\theta|y) = \frac{p(\theta,y)}{p(y)} = \frac{p(\theta)~p(y|\theta)}{p(y)}$$

An equivalent form of this omits the factor $p(y)$ because $y$ is fixed and $p(y)$ does not depend on $\theta$, which yields the unnormalized posterior density that is proportional to the product of the likelihood and the prior: $$p(\theta|y) \propto p(y|\theta)~p(\theta)$$

## Example: Binomial Data

Let $\theta$ represent the probability of success in each Bernoulli trial. The model for the number of successes $y$ in $n$ trials is the Binomial model: $$y|\theta~\sim~\text{Binom}(n,\theta)$$

### Estimating $\theta$:

- **Frequentist (Classical)** approach:
- $\hat{\theta} = \frac{y}{n}$ is an unbiased estimator of fixed parameter $\theta$
- As $n \rightarrow N$, $\hat{\theta} \rightarrow \theta$

- **Bayesian** approach:
- Interested in distribution of r.v. parameter $\theta$ given the data $y$
- The mean of posterior distribution can be used as a point estimate
- Apply Bayes' rule to get: $p(\theta~|~y) \propto p(y~|~\theta)~p(\theta)$
- We know $p(y~|~\theta)$, but what is $p(\theta)$?

## Conjugacy

A *conjugate prior* is a prior distribution that yields a posterior distribution in the same family. This is rarely the case, but makes for clean, minty maths.

| Prior | Likelihood | Posterior |
|:-----:|:----------:|:---------:|
| Beta  | Binomial   | Beta      |
| Beta | Negative Binomial | Beta |
| Gamma | Poisson | Gamma |
| Beta | Geometric | Beta |
| Gamma | Exponential | Gamma |
| Normal | Normal (mean unknown) | Normal |
| Inverse Gamma | Normal (variance unknown) | Inverse Gamma |
| Dirichlet | Multinomial | Dirichlet |

## Example cont'd: Beta-Binomial Model

Beta is a *conjugate prior* for the Binomial likelihood, so the posterior is also a Beta.

- Binomial likelihood: $p(y~|~n, \theta) = \dbinom{n}{y} \theta^y (1-\theta)^{n-y}$
- Beta prior: $p(\theta) = p(\theta~|~\alpha, \beta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{\text{B}(\alpha, \beta)},$ where $\text{B}(\alpha, \beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}$ and $\Gamma(x) = (x-1)!$
- Posterior: $p(\theta~|~y) \propto p(y|\theta)~p(\theta)$
$$p(\theta|y) \propto \dbinom{n}{y} \theta^y (1-\theta)^{n-y} \times \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{\text{B}(\alpha, \beta)},$$ which we can simplify to: $$p(\theta~|~y) \propto \theta^{(y+\alpha)-1} (1-\theta)^{(n-y+\beta)-1}$$

...which looks like the probability density function for the Beta distribution, sans normalizing constant! Therefore: $$\theta~|~y~\sim~\text{Beta}(y+\alpha, n-y+\beta)$$

## Hyperparamaters, hyper-hyperparameters, and oh my!

**Problem**: we need to specify the hyperparameters $\alpha$ and $\beta$.

**Solutions**:

- *Noninformative flat prior*: pick $\text{Beta}(\alpha = 1, \beta = 1)$ which is equivalent to $\text{Uniform}(0, 1)$
- *Jeffreys prior*: noninformative, but outside the scope of this talk
- *Informative prior*: pick values that reflect your prior belief about the shape of $p(\theta)$
- *Hierarchical model*: assign a prior distribution (a "hyperprior") to $\alpha$ and $\beta$ so posterior becomes: $$p(\theta~|~y) \propto p(y~|~\theta, \alpha, \beta) \times p(\theta~|~\alpha, \beta) \times p(\alpha~|~\beta) \times p(\beta)$$ which utilize hyper-hyperparameters for $p(\alpha)$ and $p(\beta)$

Let's see how our choice of hyperparameters affects the posterior...

## The Prior-Data-Posterior Relationship
```{r, echo = FALSE}
normalize_density <- function(x) {
  return(x/max(x))
}
shinyApp(
  ui = fluidPage(
    fluidRow(
      column(sliderInput('a_2', 'α', 0, 100, step = 1, value = 2), width = 3),
      column(sliderInput('b_2', 'β', 0, 100, step = 1, value = 2), width = 3),
      column(actionButton('flip', 'Flip a coin!'),
             actionButton('reset', 'Reset'), br(),
             actionButton('flip_10', 'Flip 10 coins!'), width = 3),
      column(p('P(Heads) = 0.64'), width = 3)
    ),
    plotOutput('update_plot')
  ),
  server = function(input, output) {
    prob_head <- 0.64
    flips <- reactiveValues(total = 0, heads = 0)
    observe({
      input$flip
      # Update data
      if (input$flip > 0) {
        current_flip <- rbinom(1, 1, prob_head)
        flips$total <- isolate(flips$total) + 1
        flips$heads <- isolate(flips$heads) + current_flip
      }
    })
    observe({
      input$flip_10
      # Update data
      if (input$flip_10 > 0) {
        current_flips <- rbinom(10, 1, prob_head)
        flips$total <- isolate(flips$total) + 10
        flips$heads <- isolate(flips$heads) + sum(current_flips)
      }
    })
    observe({
      input$reset
      # Reset data
      flips$total <- 0
      flips$heads <- 0
    })
    output$update_plot <- renderPlot({
      x <- seq(0, 1, 0.01)
      # Likelihood
      likelihood <- normalize_density(sapply(x, function(theta) {
        dbinom(flips$heads, flips$total, prob = theta)
      }))
      # Prior
      a_prior <- input$a_2
      b_prior <- input$b_2
      y_prior <- normalize_density(dbeta(x, a_prior, b_prior))
      # Posterior
      a_post <- a_prior + flips$heads
      b_post <- b_prior + flips$total - flips$heads
      y_post <- normalize_density(dbeta(x, a_post, b_post))
      # Visualize!
      plot(x, y_prior, main = "Prior and Posterior",
           xlab = expression(theta), ylab = "density (normalized for visualization)",
           type = "l", lty = "dotted", yaxt = "n", ylim = c(0, 1), lwd = 2)
      # Credible Interval
      x_lower <- qbeta(0.025, a_post, b_post)
      x_upper <- qbeta(0.975, a_post, b_post)
      x2 <- seq(x_lower, x_upper, length.out = 1000)
      polygon(c(x_lower, x2, x_upper, x_lower),
              c(0, normalize_density(dbeta(x2, a_post, b_post)), 0, 0),
              col = rgb(0.5, 0.5, 0.5, 0.25))
      # Densities
      abline(v = prob_head, lty = "solid", lwd = 2, col = "blue")
      abline(v = flips$heads/flips$total, lty = "dashed", lwd = 2, col = "red")
      lines(x, likelihood, lty = "dashed", lwd = 2, col = "red")
      lines(x, y_post, lwd = 2)
      legend("topleft", c(sprintf("Prior: Beta(%.0f,%.0f)", a_prior, b_prior),
                          sprintf("Likelhood & sample prop based on %.0f heads in %.0f flips", flips$heads, flips$total),
                          sprintf("Posterior: Beta(%.0f,%.0f)", a_post, b_post)),
             lty = c("dotted", "dashed", "solid"), lwd = 2,
             col = c("black", "red", "black"))
    })
  }, options = list(width = 800)
)
```

## Updating Posterior

## Demonstration

```{r, echo = FALSE}
```
